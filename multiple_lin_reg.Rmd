---
title: "Homework 9"
output: html_document
author: Akshay Gona
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Problem 1 More regression with `mtcars` (12 points; 2 pts each)

In lecture, we worked briefly with the `mtcars` data set.
Let's get more regression practice by working with it some more.

### a) background

Run `?mtcars` in the console (please __do not__ add it to this `Rmd` file) and briefly read the help page.
Specifically, take note of the following:

1. What is the source of this data?
2. What is this data set measuring (i.e., what was the response variable in the original study, at least based on the brief description in the R documentation)?
3. What predictors are available and what do they mean?

***

This data comes from the 1974 Motor Trend US magazine. 

The response variable for the dataset is mpg (Miles/gallon).

The predictors are:
cyl(Number of cylinders)
disp(Displacement (cu.in.))
hp(Gross horsepower) 
drat(Rear axle ratio) 
wt(Weight(1000 lbs)) 
qsec(1/4 mile time)
vs(Engine (0 = V-shaped, 1 = straight))
am(Transmission (0 = automatic, 1 = manual))
gear(Number of forward gears)
carb(Number of carburetors)

***

You may want to also run `head(mtcars, 10)` or `View(mtcars)` to inspect the data frame briefly before moving on.

### b) Fitting a model

Use `lm` to run a regression of `mpg` on a few predictors in the data frame (choose two or three that you think would make a good model-- don't use all ten; we'll talk about why in later lectures).
Make sure to include `data = mtcars` as a keyword argument to `lm` so that R knows what data frame to use.

```{r}
# lm.mtcars = lm(...)
lm.mtcars = lm(mpg ~ hp + disp, data = mtcars)
plot(lm.mtcars,ask=F,which=1:2)
```

Briefly inspect the residuals plot by running `plot(lm.mtcars,ask=F,which=1:2)`.
What do you observe, and what does it mean?

***

We can see that there is a bit of a pattern with the residuals in the residuals v fitted plot and the q-q residuals plot looks normal. Linear regression should be fine here.
***

### c) Interpreting the model

View the summary of your model by uncommenting and running the code below.
```{r}
summary(lm.mtcars)
```

Pick one of your predictors and give an interpretation of the estimate and standard error for its coefficient.
Be careful in your wording of the interpretation.

***
The estimate for the coefficient of hp is -0.024840 with a standard error of 0.013385. With an increase in disp by 1, we should expect to see a -0.024840 unit increase in cubic inches. The standard error implies that this value is expected to vary by 0.013385.

***

Which coefficients are statistically significantly different from zero? How do you know?

***

Disp is statistically significant from 0, but hp is not, and we can tell from p value being greater or less than 0.05.

***

### d) Interpreting residuals

What is the Residual Standard Error (RSE) for this model? How many degrees of freedom does it have?

***

3.127 on 29 degrees of freedom

***

What is the value of $R^2$ for this model? (__Hint:__ look at the output of `summary`) Give an interpretation of this value.

***

0.7482; This is a measure of how well the predictor variables explain the variability of the response variable and shows the proportion of the variance in the response variable that is predictable from the predictor variables.

***

### e) Adjusted $R^2$

Briefly read about the adjusted $R^2$ [here](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/adjusted-r2/).
What is the adjusted $R^2$ of this model and how does this differ from the usual $R^2$ value? (__Hint:__ again, look at the output of `summary`).

***

Adjusted R-squared:0.7309; This value changes as it adjusts for the number of terms in the model. More irrevalant predictors indicate that this value will decrease. With more useful predictors, it increases. As the R-squared decreases, we can tell that the predictors might not be as useful.

***

### f) CIs for coefficients

Read the documentation for the `confint` function, and use it to generate $95\%$ confidence intervals for the coefficients of your model.
Give an interpretation of these confidence intervals.

```{r}
confint(lm.mtcars,level = 0.95)
```

***

We are 95% confident that the intercept (expected when hp and disp are 0) is between 28.01254573 and 33.459262767, the coefficient to hp is between -0.0522165 and 0.002536338, and the coefficient to drat is between -0.04549091 and -0.015201645.

***


## Problem 2) the `cats` data set (8 points; 2pts each)

The `cats` data set, included in the `MASS` library, contains data recorded from 144 cats.
Each row of the data set contains the body weight (`Bwt`, in kgs), heart weight (`Hwt`, in grams) and the sex (`Sex`, levels `'F'` and `'M'`) for one of the cats in the data set.

### a) plotting the data

Create a scatter plot showing heart weight on the y-axis and body weight on the x-axis.
Ignore the `Sex` variable in this plot.

```{r}
library(MASS)
head(cats)
```

```{r}

plot(Bwt ~ Hwt, data = cats, xlab = "Weight", ylab = "Heart Weight", main = "Weight vs Heart Weight")

```

Briefly describe what you see. Is there a clear trend in the data?

**Positive correlation between the two variables, increase in overall weight seems to mean increase in heart weight.**

### b) fitting a linear model

Fit a linear regression model to predict cat heart weight from cat body weight (and using an intercept term, of course).

```{r}
model = lm(Hwt ~ Bwt, data = cats)
```

Examine the coefficients of your fitted model.
What is the coefficient for the `Bwt` variable?
Interpret this coefficient-- a unit change in body weight yields how much change in heart weight?

```{r}
summary(model)
```

***

The coefficient for the Bwt variable is 4.0341. With a one unit increase in overall weight, should see 4.0341 unit increase in heart weight. 

***

### c) back to plotting

Create the same plot from Part a above, but this time color the points in the scatter plot according to the `Sex` variable.
You may use either `ggplot2` or the built-in R plotting tools, though I would recommend the former, for this.

You should see a clear pattern. Describe it. A sentence or two is fine here.

```{r}
library(ggplot2)
ggplot(cats, aes(x = Bwt, y = Hwt, color = Sex)) +
  geom_point() +
  labs(x = "Weight", y = "Heart Weight", title = " Weight vs Heart Weight")
```

***

Male cats have greater overall body and heart weights than female cats.

***

### d) adding `Sex` and an interaction

From looking at the data, it should be clear that the `Sex` variable has explanatory power in predicting heart weight, but it is also very correlated with body weight.

Fit a new linear regression model, still predicting heart weight, but this time including both body weight and sex as predictors *and* an interaction term between body weight and sex.
Take note of how R assigns `Sex` a dummy encoding.

```{r}
model2 = lm(Hwt ~ Bwt + Sex + Bwt:Sex, data = cats)
summary(model2)
```

Examine the outputs of your model.
In particular, note the coefficients of `Sex` and the interaction between `Bwt` and `Sex`.
Are both of these coefficients statistically significantly different from zero?
How do you interpret the interaction term?

***

The SexM coefficient helps us understand why male and female cats might have different heart weights when we consider their body weights. Meanwhile, the Bwt:SexM interaction term looks at how the connection between body weight and heart weight changes depending on whether the cat is male or female. Both these coefficients are important because their p-values are less than 0.05, showing they matter in the model. Specifically, the Bwt:SexM coefficient, at 1.6763, tells us that in male cats, body weight has about 1.6763 grams more impact on heart weight compared to female cats.

***


## Problem 3 - Using Multiple regression to fit nonlinear data (10 points, 2.5 pts each)

Open the dataset `multData.csv`. This data set consists of three predictor variables, simply named `X1`, `X2` and `X3`. The response variable is `Y`. In this problem you will explore how to use the multiple regression model to model nonlinear relationships.

### a) the first model

First we will explore the relationship between $Y$ and the first two predictors $X1$ and $X2$. Fit the linear model

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$$
Interpret the coefficients of both X1 and X2. 

```{r}
library(readr)
mData <- read_csv("multData.csv")
model <- lm(Y ~ X1 + X2, data = mData)
summary(model)
```

***

The coefficient for X1 is -6.7573, for a unit rise in X1, predicted value of Y drops by 6.7573 units while X2 stays the same. On the other hand, the coefficient for X2 is -22.8693; each one-unit increase in X2, expected value of Y decreases by 22.8693 units and X1 stays constant.

***


### b) Investigating interaction of quantitative predictors

Next introduce an interaction term to the model
$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1\cdot X_2 + \epsilon$$

Fit the model and view the summary output. Has this improved the model fit? Did anything surprising happen to the coefficients? Try to explain what happened.


```{r}
model2 <- lm(Y ~ X1 + X2 + X1:X2, data = mData)
summary(model2)
```

***

The model has improved slightly as R^2 increased and RSE decreased. However, both X1 and X2 have become positive when they were previously negative. This may be since  the interaction term allows the effects of both X1 and X2 combined to have an effect on the linear model, and without the interaction term, the two terms were acting independent.

***


### c) Introducing the last predictor

Next fit a model that introduces the `X3` variable. 

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1\cdot X_2  + \beta_4 X_3 \epsilon$$
Has the model fit improved? In what way (Justify your answer)? 

```{r}
model3 = lm(Y ~ X1 + X2 + X1:X2 + X3, data = mData)
summary(model3)
```

***

The model has significantly improved, R^2 increased a lot and RSE dropped a lot.

***


### d) Considering higher order terms

Finally explore higher order terms for the X3 variable: Introduce $X3^2$, $X3^3$ etc and determine if any of these higher order terms are justified in the model. Explain your reasoning and present your final model. Look at the diagnostic plots and discuss whether the assumptions of the multiple regression model seem to be justified.

```{r}
model4 <- lm(Y ~ X1 + X2 + X1:X2 + I(X3^2), data = mData)
summary(model4)
model5 <- lm(Y ~ X1 + X2 + X1:X2 + I(X3^3), data = mData)
summary(model5)
final_model <- lm(Y ~ X1 + X2 + X1:X2 + X3, data = mData)
summary(final_model)
plot(final_model)
```

***

No higher order model is better/justified; the squared model has both a lower R^2 and greater RSE. For the cubed model, though the R^2 is slightly higher, the RSE also increases. The final model will just use X3 in the single order. The residual plot of the final model has residuals with no pattern centered on 0. On the 1-1 plot, they follow a diagonal line and the points are spread well for the fitted values, and greater leverage points don't affect the residuals. The assumptions made are justified.

***

