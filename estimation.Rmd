---
title: "Homework 6"
author: "Akshay Gona"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Problem \#1: Estimating Quantiles <small>(8 pts; 2pts each)</small>

There are 9 algorithms in R to estimate population quantiles. Type `?quantile` to read about them. Here we will investigate the variance of some of these estimators. To use the quantile function you use the syntax
`quantile(vector, probs, type)`.
For example if you have data in a vector called `sampleData` and you wish to estimate the 80th percentile using algorithm 7 (the default), you use
`quantile(sampleData, .80, type=7)`

Suppose we're interested in the 95th percentile for $X$, and we know that $X$ follows a uniform distribution. We want to randomly sample $n=30$ values and estimate the 95th percentile. Using MC simulation estimate the following:

a. Which quantile algorithm (4 through 9 has the smallest absolute bias? *Hint: you can use $unif(0,1)$ for the purposes of this estimation, as your answer won't depend on the upper and lower bounds chosen.*
```{r}
NMC <- 1000
bias <- function(estimates, true_value) {
  mean(estimates) - true_value
}
simbias <- function(type) {
  biases = NMC
  for (i in 1:NMC) {
    sampleData <- runif(30)
    estimates <- quantile(sampleData, 0.95, type = type)
    biases[i] <- mean(estimates) - 0.95
  }
  mean(biases)
}

biases <- sapply(4:9, simbias)
abs(biases)
```

b. Which quantile algorithm (4 through 9) has the smallest variance?
```{r}
simvariance <- function(type) {
  variances <- numeric(NMC)
  for (i in 1:NMC) {
    sampleData <- runif(30)
    estimates <- quantile(sampleData, 0.95, type = type)
    variances[i] <- estimates
  }
  var(variances)
}
variances <- sapply(4:9, simvariance)
variances
```

Type 6 is has the smallest variance.

c. Which method is best for estimating the 95th percentile from a uniform distribution? Justify your answer.

Type 6 has lowest variance and bias, is the best for estimating the 95th percentile from a uniform distribution.

d. What about if $X\sim N(\mu, \sigma^2)$? Would you prefer a different method for estimating the 95th percentile from a normal distribution? *Hint: repeat the same analysis for $N(0,1)$.*

```{r}
NMC <- 10000
bias <- function(estimates, true_value) {
  mean(estimates) - true_value
}

simbias <- function(type) {
  biases = NMC
  for (i in 1:NMC) {
    sampleData <- rnorm(30)
    estimates <- quantile(sampleData, 0.95, type = type)
    biases[i] <- mean(estimates) - 0.95
  }
  mean(biases)
}

biases <- sapply(4:9, simbias)
which.min(abs(biases))

simvariance <- function(type) {
  variances <- numeric(NMC)
  for (i in 1:NMC) {
    sampleData <- rnorm(30)
    estimates <- quantile(sampleData, 0.95, type = type)
    variances[i] <- estimates
  }
  var(variances)
}

variances <- sapply(4:9, simvariance)
which.min(variances)
```


First value is lowest, and it represents type 4, so it is the best for normal distribution.


## Problem \#2: Estimating a Geometric $p$ <small>(6 pts; 2 pts each)</small>

a. Use the method of moments to come up with an estimator for a geometric distributions parameter $p$. *Hint: Use the fact that if $X\sim Geom(p)$ then $EX=\frac{1-p}{p}$. 

$EX = \frac{1}{n} \sum{i=1}^n X_i$

$\frac{1-p}{p} = \frac{1}{n} \sum{i=1}^n Xi$

$\frac{1}{p} = 1 + \frac{1}{n} \sum{i=1}^n Xi$

$p = \frac{1}{1 + \frac{1}{n} \sum{i=1}^n X_i}$

b. Estimate the sampling distribution of this estimator when we sample $n=13$ values from from $Geom(.15)$. Show the histogram of the estimated sampling distribution.
```{r}
NMC <- 1000
estimators <- numeric(NMC)

for (i in 1:NMC) {
  sample_data <- rgeom(13, 0.15)
  estimator <- 1 / (1 + mean(sample_data))
  estimators[i] <- estimator
}

hist(estimators, main="Estimator Distribution", xlab="Estimator", ylab="Freq")
```

c. Estimate the bias of this estimator. Is it biased? If it is biased how would you modify it so that you could create an unbiased estimator?

Bias seems pretty low, we can say that it is an unbiased estimator.


## Problem \#3: Estimating $\lambda$ from a Poisson Distribution<small>(8 pts; 2 pts each)</small>

It is interesting that if $X\sim Pois(\lambda)$ that $EX=VarX=\lambda$. One could use either $\bar{X}$ or $S^2$ as an estimator of $\lambda$ perhaps. 

a. Using $n=15$ and $\lambda=20$ for this problem, use MC simulation to estimate the sampling distribution of The estimator $\bar{X}$. Show its histogram. 
```{r}
NMC <- 10000
sample_means <- numeric(NMC)
for (i in 1:NMC) {
  samples <- rpois(15, 20)
  sample_means[i] <- mean(samples)
}

hist(sample_means, main = "Sampling Distribution of X_bar",
     xlab = "Sample Mean", ylab = "Frequency",)
```

b. Repeat the same but this time use $S^2$. 
```{r}
sample_vars <- numeric(NMC)
for (i in 1:NMC) {
  samples <- rpois(15, 20)
  sample_vars[i] <- var(samples)
}

hist(sample_vars, main = "Sampling Distribution of the Estimator S^2",
     xlab = "Sample Variance", ylab = "Frequency")
```

c. Compare the two estimators. Would you prefer one over the other? Why?

X_bar as an estimator seems better since Variance values had a much larger range than X_bar.

d. What about a linear combination of the two variables? Could you construct an estimator of $\lambda$ of the form $a\bar{X} + bS^2$ that would be better than using either of them by themselves? 

Yes, it's possible to construct a linear combination of Xbar and variance to form a better estimator of lambda. We could use the method of moments, set the first two moments of the estimator equal to the first two moments of the parameter that is being estimated.This estimator combines the sample mean and sample variance in a weighted manner, where the weights are inversely proportional to the sample size. It could provide a better estimator than either one of them alone:

```{r}
#ask TA, no idea if this is right
#lambda <- (1 - 1/n) * mean(samples) + 1/n * var(samples)
```


## Problem \#4: The Standard Error of $\bar{X}$<small>(8 pts; 2 pts each)</small>

What would be the required sample size $n$ so that the standard error of $\bar{X}$ (i.e. $SD(\bar{X})$) would be 2 (or just under 2) for the following populations:

a. $\text{Normal}(1000, 10^2)$
```{r}
ceiling(10^2 / 2^2)
```

b. $\text{Poisson}(75)$
```{r}
ceiling(75^2 / 2^2)
```

c. $\text{Binomial}(200, .35)$
```{r}
ceiling((200 * .35 * .65)^2 / 2^2)
```

d. $\text{Exponential}(.05)$
```{r}
ceiling(((1 / (0.05^2))^2 / 2^2))
```



